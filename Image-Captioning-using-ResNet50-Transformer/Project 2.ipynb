{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Project 2: First approaches to MultiModal Transformers: Bridging Text with Vision, Audio, and Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract",
   "metadata": {},
   "source": [
    "# Objective: \n",
    "\n",
    "Instead of treating text, audio, and video as separate streams of information, you will design a **Transformer-based model that intelligently fuses two modalities**—text with images, text with audio, or text with video. Your challenge is to harness the power of deep learning to create a system where each modality enhances the other, unlocking richer, more meaningful insights.\n",
    "\n",
    "This is more than just training a model—it’s about innovation. How will you design a fusion strategy that truly captures cross-modal relationships? Will your model generate creative text from images, answer questions from audio, or retrieve videos based on descriptions? The decisions are yours to make.\n",
    "\n",
    "Even if you and your peers work with similar datasets, your approach must be unique. Whether through data choices, architectural modifications, or fusion techniques, your model should push the boundaries of multimodal AI. Experiment boldly, optimize strategically, and most importantly—create something exciting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22115c68",
   "metadata": {},
   "source": [
    "# Deliverables:\n",
    "\n",
    "- A working model (hybrid architecture)\n",
    "- A structured report (including visuals & reflections) **Required: Include details about your hybrid architecture!!!!**\n",
    "- A GitHub repository with clean, documented code\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c0842f",
   "metadata": {},
   "source": [
    "# Step 1: Select your own adventure\n",
    "\n",
    "Below is a concise, high-level breakdown of three main multimodal “adventures,” each with several task options and brief notes about potential datasets and implementation tips. This structure makes it easy to pick a project that best fits your interests and available resources—whether you prefer images, audio, or video combined with text. **Required in your reports regardless of the choice picked: Include details about your hybrid architecture!!!!**\n",
    "\n",
    "## **Choice 1: Images + Text**\n",
    "\n",
    "### 1. **Image Captioning**\n",
    "- **Goal:** Automatically generate textual descriptions (captions) for given images.  \n",
    "- **Potential Datasets:**  \n",
    "  - **MS COCO** – Large-scale, ~330k images with multiple captions per image.  \n",
    "  - **Flickr8k/30k** – Smaller datasets; useful for quick iteration.  \n",
    "- **Implementation Tips:**  \n",
    "  - Use a **CNN or Vision Transformer** to encode images, then a Transformer decoder for generating text.  \n",
    "  - Evaluate output with **BLEU, METEOR, or CIDEr**.\n",
    "\n",
    "### 2. **Visual Question Answering (VQA)**\n",
    "- **Goal:** Answer open-ended questions about image content (e.g., “How many dogs are in this picture?”).  \n",
    "- **Potential Datasets:**  \n",
    "  - **VQA v2** – 204k images and ~1 million Q&A pairs.  \n",
    "  - **GQA** – Emphasizes compositional reasoning.  \n",
    "- **Implementation Tips:**  \n",
    "  - Fuse **image features** (from a CNN/ViT) with **question embeddings** (Transformer for text).  \n",
    "  - Evaluate with **accuracy** for classification-based answers or **language metrics** for open-ended answers.\n",
    "\n",
    "\n",
    "### 3. **Image-Text Retrieval**\n",
    "- **Goal:** Retrieve the most relevant images given a text query, or vice versa.  \n",
    "- **Potential Datasets:**  \n",
    "  - **MS COCO** – Commonly used for both captioning and retrieval.  \n",
    "  - **Flickr30k** – Includes structures suited to retrieval tasks.  \n",
    "- **Implementation Tips:**  \n",
    "  - Use **dual encoders** for image and text, trained with a **contrastive loss** to align modalities.  \n",
    "  - Evaluate with **Recall@K** or **mean rank** metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## **Choice 2: Audio + Text**\n",
    "\n",
    "### 1. **Speech Recognition**\n",
    "- **Goal:** Convert spoken language (waveforms) into written text transcripts.  \n",
    "- **Potential Datasets:**  \n",
    "  - **LibriSpeech** – ~1,000 hours of English audiobook recordings.  \n",
    "  - **Mozilla Common Voice** – Crowd-sourced, multilingual speech data.  \n",
    "- **Implementation Tips:**  \n",
    "  - Convert waveforms into **Mel spectrograms**, or use **wav2vec2** (pretrained).  \n",
    "  - Evaluate with **Word Error Rate (WER)**.\n",
    "\n",
    "### 2. **Audio-Text Alignment**\n",
    "- **Goal:** Match spoken words or segments in an audio file to their written transcripts (often down to timestamps).  \n",
    "- **Potential Datasets:**  \n",
    "  - **TEDLIUM** – TED talks with aligned transcripts.  \n",
    "  - **YouTube** auto-transcripts (though noisier).  \n",
    "- **Implementation Tips:**  \n",
    "  - Segment audio frames; align with text tokens.  \n",
    "  - Use **CTC-based** approaches or techniques like Dynamic Time Warping (DTW).  \n",
    "  - Applications: **karaoke-style** subtitles, real-time captioning.\n",
    "\n",
    "\n",
    "### 3. **Spoken Command Classification**\n",
    "- **Goal:** Identify short, predefined voice commands like “Turn on the light.”  \n",
    "- **Potential Datasets:**  \n",
    "  - **Google Speech Commands** – Tens of thousands of short utterances for specific commands.  \n",
    "- **Implementation Tips:**  \n",
    "  - A **classification task** (label each audio clip with the intended command).  \n",
    "  - Evaluate with **accuracy** or **F1 score**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Choice 3: Video + Text**\n",
    "\n",
    "### 1. **Video Captioning**\n",
    "- **Goal:** Generate textual descriptions for short videos (e.g., “A person cooking pasta”).  \n",
    "- **Potential Datasets:**  \n",
    "  - **MSR-VTT** – ~10k short video clips, each with multiple captions.  \n",
    "  - **YouCook2** – Cooking videos with detailed instructions.  \n",
    "- **Implementation Tips:**  \n",
    "  - Sample frames (e.g., 1 fps) for each video.  \n",
    "  - Encode frames (CNN/ViT) and use a Transformer decoder for text.  \n",
    "  - Evaluate with **BLEU, METEOR, or CIDEr**.\n",
    "\n",
    "\n",
    "### 2. **Video Question Answering (Video QA)**\n",
    "- **Goal:** Answer questions based on video content (objects, actions, context).  \n",
    "- **Potential Datasets:**  \n",
    "  - **TVQA** – TV show clips plus questions about dialogue and visuals.  \n",
    "  - **LSMDC** – Movie clips with descriptions/questions.  \n",
    "- **Implementation Tips:**  \n",
    "  - Extract **visual features** from sampled frames; optionally include **subtitles/transcripts**.  \n",
    "  - Fuse them with question embeddings in a multimodal Transformer.  \n",
    "  - Evaluate with **accuracy** or open-ended **language metrics**.\n",
    "\n",
    "\n",
    "### 3. **Text-Based Video Retrieval**\n",
    "- **Goal:** Find relevant video clips from a database based on a text query (e.g., “Videos of someone playing guitar”).  \n",
    "- **Potential Datasets:**  \n",
    "  - **MSR-VTT** – Contains clips plus textual metadata.  \n",
    "  - **ActivityNet Captions** – Videos with temporal captions.  \n",
    "- **Implementation Tips:**  \n",
    "  - Use **dual encoders** or a **joint embedding** space.  \n",
    "  - Evaluate with **Recall@K**, **MRR** (Mean Reciprocal Rank), or similar retrieval metrics.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9ba828-2616-4525-a8ca-5c85fe2f57db",
   "metadata": {},
   "source": [
    "## **General Reccomendations**\n",
    "\n",
    "\n",
    "#### 1. **Transformer Architecture**\n",
    "\n",
    "- **Separate Encoders:** Build one encoder for text and another for your chosen modality. Fuse the resulting embeddings either through cross-attention or by concatenating them, then feeding them into further layers.\n",
    "\n",
    "- **Learned Modality Embeddings:** Introduce special learned tokens (e.g., [IMAGE], [AUDIO], [VIDEO]) to flag which modality a token or embedding belongs to. This can help the Transformer distinguish between, say, a text token vs. an image patch embedding.\n",
    "\n",
    "- **Cross-Attention:** If you’re using an encoder–decoder structure (common for generation tasks like captioning), the decoder can attend to both text representations and other modality representations. This is especially potent if your final output is text (e.g., describing an image or transcribing an audio snippet).\n",
    "\n",
    "- **Positional or Spatial Embeddings:**\n",
    "Images/Videos: 2D positional embeddings to capture spatial layout.\n",
    "Audio: Time–frequency positional embeddings to reflect temporal progression.\n",
    "Text: Standard 1D positional embeddings or relative positioning can suffice.\n",
    "\n",
    "#### 2. **Fusion Strategy for Multimodality**\n",
    "\n",
    "- **Concatenation:** The simplest method—just stack text embeddings and modality embeddings along the sequence dimension. Make sure each chunk has a clear positional signal.\n",
    "\n",
    "- **Attention-based Fusion:**\n",
    "Let each modality have its own encoder.\n",
    "Combine them via cross-attention in later layers, where the text representation attends to the image/audio/video representation or vice versa.\n",
    "You might even try mutual cross-attention for an even richer representation.\n",
    "\n",
    "- **Late Fusion**: Encode each modality separately, then merge the final embeddings (e.g., by averaging, concatenation, or a learnable projection) to feed into a classification or decoding head.\n",
    "\n",
    "#### 3. **Training Loop and Objective**\n",
    "\n",
    "- **Loss Functions**\n",
    "Text Generation (e.g., captioning): Cross-entropy on the predicted tokens.\n",
    "Classification (VQA, spoken command classification): Cross-entropy or binary cross-entropy.\n",
    "Retrieval (matching text to images/ audio/video): Contrastive or triplet loss.\n",
    "\n",
    "- **Masking**\n",
    "Carefully handle [PAD] tokens so the attention mechanism ignores those placeholders. Use key padding masks in PyTorch for both the source and target.\n",
    "\n",
    "- **Training Details**\n",
    "Use AdamW or a similar optimizer with a suitable learning rate scheduler (e.g., warmup + decay).\n",
    "Watch your GPU memory usage. If your model or data is large, consider gradient checkpointing or reduce batch size.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "Each **Choice** (Images + Text, Audio + Text, or Video + Text) comes with **three distinct tasks** of escalating complexity. Select the modality and task that excite you most and that fit your available computing resources. Focus on building a solid **data pipeline**, leveraging **pretrained models**, and performing **continuous evaluation** to ensure tangible progress over your project timeline. \n",
    "\n",
    "### **You will need to research some of the approaches reccomended here, but, believe me, that is the way real world works! Frustration is always allowed!**\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e22e37",
   "metadata": {},
   "source": [
    "## Clarification\n",
    "\n",
    "You **don't** need to develop an interactive application for this project. The demo will serve as a platform to communicate your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a5204",
   "metadata": {},
   "source": [
    "# Step 2: Submit Your Work\n",
    "\n",
    "Your submission package should include:\n",
    "\n",
    "1. **GitHub Repository** (Well-documented code). ``add`` and ``commit`` the final version of your work, and ``push`` your code to your GitHub repository. You can have multiple notebooks. It is up to you.\n",
    "2. **Project Report** – 4-page IEEE-format paper. Write a paper with no more than 4 pages addressing the architecture, tasks outcomes and reflections. When writing this report, consider a business-oriented person as your reader (e.g. your PhD advisor, your internship manager, etc.). Tell the story for each datasets' goal and tasks covered. **Required: Include details about your hybrid architecture!!!!** Also, include insights about:\n",
    "- Significance of your implementation\n",
    "- Accuracy, loss curves, feature importance.\n",
    "- What worked, what didn’t, what’s next?\n",
    "- Where could this be applied?\n",
    "\n",
    "3. **Demo Link or Video** (Showcasing your model’s workflow)\n",
    "4. **README.md file.** Edit the readme.md file in your repository and how to use your code. Ensure reproducibility (environment requirements, requirements.txt, or environment.yml)\n",
    "\n",
    "\n",
    "**``Submit the URL of your GitHub Repository as your assignment submission on Canvas.``**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedf7c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
